{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission Atlantic - CS SOMAR\n",
    "## Data treatment for ICCAT T2CE database\n",
    "### author: Henrique Amato Peres - ha.peres@usp.br\n",
    "#### 11 March 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open .csv file containing ICCAT T2CE database sliced geographically to CS SOMAR area and surroundings, between 45째W and 10째E and between 30째S and 15째N. That decreases file size from 688 to 112 MB. Then load it as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iccat_t2ce_SAt_allgr_3nov21.csv') as csvfile:\n",
    "    f = list(csv.reader(csvfile))\n",
    "\n",
    "headers = f[0]\n",
    "df = pd.DataFrame(f[1:], columns=headers)\n",
    "del(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data types for each column and slice to begin in 2012, similar to the temporal range of the Global Fishing Watch database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = ['BFT', 'ALB', 'YFT', 'BET', 'SKJ', 'SWO', 'BUM', 'SAI', 'SPF', 'WHM', 'BLF', 'BLT', 'BON', \n",
    "                'BOP', 'BRS', 'CER', 'FRI', 'KGM', 'LTA', 'MAW', 'SLT', 'SSM', 'WAH', 'DOL', 'BIL', 'BLM', \n",
    "                'MSP', 'MLS', 'RSP', 'SBF', 'oTun', 'BSH', 'POR', 'SMA', 'oSks']\n",
    "\n",
    "for var in ['Eff1', 'Eff2', 'BFT', 'ALB', 'YFT', 'BET', 'SKJ', 'SWO', 'BUM', 'SAI', 'SPF', 'WHM', 'BLF', \n",
    "            'BLT', 'BON', 'BOP', 'BRS', 'CER', 'FRI', 'KGM', 'LTA', 'MAW', 'SLT', 'SSM', 'WAH', 'DOL', \n",
    "            'BIL', 'BLM', 'MSP', 'MLS', 'RSP', 'SBF', 'oTun', 'BSH', 'POR', 'SMA', 'oSks']:\n",
    "    df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "df = df.astype({'StrataID': 'int64', 'DSetID': 'int64', 'FleetID': 'string', 'GearGrpCode': 'string', \n",
    "                'GearCode': 'string', 'FileTypeCode': 'string', 'YearC': 'int64', 'TimePeriodID': 'int64', \n",
    "                'SquareTypeCode': 'string', 'QuadID': 'int64', 'Lat': 'float64', 'Lon': 'float64', \n",
    "                'SchoolTypeCode': 'string', 'Eff1': 'float64', 'Eff1Type': 'string', 'Eff2': 'float64', \n",
    "                'Eff2Type': 'string', 'DSetTypeID': 'string', 'CatchUnit': 'string', 'BFT': 'float64', \n",
    "                'ALB': 'float64', 'YFT': 'float64', 'BET': 'float64', 'SKJ': 'float64', 'SWO': 'float64', \n",
    "                'BUM': 'float64', 'SAI': 'float64', 'SPF': 'float64', 'WHM': 'float64', 'BLF': 'float64', \n",
    "                'BLT': 'float64', 'BON': 'float64', 'BOP': 'float64', 'BRS': 'float64', 'CER': 'float64', \n",
    "                'FRI': 'float64', 'KGM': 'float64', 'LTA': 'float64', 'MAW': 'float64', 'SLT': 'float64', \n",
    "                'SSM': 'float64', 'WAH': 'float64', 'DOL': 'float64', 'BIL': 'float64', 'BLM': 'float64', \n",
    "                'MSP': 'float64', 'MLS': 'float64', 'RSP': 'float64', 'SBF': 'float64', 'oTun': 'float64', \n",
    "                'BSH': 'float64', 'POR': 'float64', 'SMA': 'float64', 'oSks': 'float64'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['YearC'] > 2011]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend for species code:\n",
    "<br/><br/>\n",
    "\n",
    "##### Tuna (major species)\n",
    "\n",
    "BFT: Atlantic bluefin tuna, *Thunnys thynnus*\n",
    "\n",
    "ALB: Albacore, *Thunnus alalunga*\n",
    "\n",
    "YFT: Yellowfin tuna, *Thunnus albacares*\n",
    "\n",
    "BET: Bigeye tuna, *Thunnus obesus*\n",
    "\n",
    "SKJ: Skipjack tuna, *Katsuwonus pelamis*\n",
    "\n",
    "SWO: Swordfish, *Xiphias gladius*\n",
    "\n",
    "BUM: Blue marlin, *Makaira nigricans*\n",
    "\n",
    "SAI: Atlantic sailfish, *Istiophorus albicans*\n",
    "\n",
    "SPF: Longbill spearfish, *Tetrapturus pfluegeri*\n",
    "\n",
    "WHM: White marlin, *Tetrapturus albidus*\n",
    "<br/><br/>\n",
    "##### Tuna (small tuna)\n",
    "\n",
    "BLF: Blackfin tuna, *Thunnus atlanticus*\n",
    "\n",
    "BLT: Bullet tuna, *Auxis rochei*\n",
    "\n",
    "BON: Atlantic bonito, *Sarda sarda*\n",
    "\n",
    "BOP: Plain bonito, *Orcynopsis unicolor*\n",
    "\n",
    "BRS: Serra Spanish mackerel, *Scomberomorus brasiliensis*\n",
    "\n",
    "CER: Cero, *Scomberomorus regalis*\n",
    "\n",
    "FRI: Frigate tuna, *Auxis thazard*\n",
    "\n",
    "KGM: King mackerel, *Scomberomorus cavalla*\n",
    "\n",
    "LTA: Little tunny (= Atl. black skipj), *Euthynnus alletteratus*\n",
    "\n",
    "MAW: West African Spanish mackerel, *Scomberomorus tritor*\n",
    "\n",
    "SLT: Slender tuna, *Allothunnus fallai*\n",
    "\n",
    "SSM: Atlantic Spanish mackerel, *Scomberomorus maculatus*\n",
    "\n",
    "WAH: Wahoo, *Acanthocybium solandri*\n",
    "\n",
    "DOL: Common dolphinfish, *Coryphaena hippurus*\n",
    "<br/><br/>\n",
    "##### Tuna (other species)\n",
    "\n",
    "BIL: Marlins, sailfishes, etc., Istiophoridae\n",
    "\n",
    "BLM: Black marlin, *Makaira indica*\n",
    "\n",
    "MSP: Mediterranean spearfish, *Tetrapturus belone*\n",
    "\n",
    "MLS: Striped marlin, *Tetrapturus audax*\n",
    "\n",
    "RSP: Roundscale spearfish, *Tetrapturus georgii*\n",
    "\n",
    "SBF: Southern bluefin tuna, *Thunnus maccoyii*\n",
    "\n",
    "oTun: Other tuna species\n",
    "<br/><br/>\n",
    "##### Sharks (major species)\n",
    "\n",
    "BSH: Blue shark, *Prionace glauca*\n",
    "\n",
    "POR: Porbeagle, *Lamna nasus*\n",
    "\n",
    "SMA: Shortfin mako, *Isurus oxyrinchus*\n",
    "\n",
    "oSks: Other shark species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for var in ['GearGrpCode', 'GearCode', 'SquareTypeCode', 'SchoolTypeCode', 'Eff1Type', \n",
    "#            'Eff2Type', 'CatchUnit']:\n",
    "#    df[var].value_counts().plot.pie(autopct='%.1f%%')\n",
    "#    plt.show()\n",
    "#    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend for gear code:\n",
    "<br/><br/>\n",
    "PS: Purse seine\n",
    "\n",
    "LL: Longline\n",
    "\n",
    "BB: Baiboat\n",
    "\n",
    "LLSWO: Longline, targetting SWO (used by Italy)\n",
    "\n",
    "HAND: Handline\n",
    "\n",
    "LL-surf: Longline, surface (used by Italy)\n",
    "\n",
    "GILL: Gillnet, drift net\n",
    "\n",
    "UNCL: Unclassified, gears not reported\n",
    "\n",
    "TRAW: Trawl\n",
    "\n",
    "LLJAP: Longline, japanese (Spain)\n",
    "\n",
    "TROL: Trolling\n",
    "\n",
    "SPOR: Sport, recreational fisheries (mostly rod and reel)\n",
    "\n",
    "TRAWP: Trawl, mid-water pelagic trawl (= MWT)\n",
    "\n",
    "HS: Haul seine\n",
    "\n",
    "PSFS: Purne seine, catching small fish\n",
    "\n",
    "SURF: Surface fisheries unclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring effort and gears in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sank = df.groupby(['SquareTypeCode', 'GearCode', 'Eff1Type', 'Eff2Type']).size().reset_index(name='counts')\n",
    "\n",
    "#sank2 = df.groupby(['GearGrpCode', 'Eff1Type', 'Eff2Type']).agg({'Eff1': ['count', 'min', 'max', 'median'], \n",
    "#                                                  'Eff2': ['min', 'max', 'median']})\n",
    "\n",
    "#pd.set_option('display.max_rows', 140)\n",
    "#print(sank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587035\n",
      "570808\n",
      "ratio = 0.97235769587844\n"
     ]
    }
   ],
   "source": [
    "n_total = df.shape[0]\n",
    "print(n_total)\n",
    "\n",
    "### Selection as descripted above\n",
    "df_s = df[(df['SquareTypeCode']=='1x1') | (df['SquareTypeCode']=='5x5')]\n",
    "del(df)\n",
    "df_s = df_s[(df_s['GearGrpCode']=='LL') | (df_s['GearGrpCode']=='BB') | (df_s['GearGrpCode']=='PS')]\n",
    "df_s = df_s[(df_s['Eff1Type']=='FISH.HOUR') | (df_s['Eff1Type']=='D.FISH') | (df_s['Eff1Type']=='NO.HOOKS') | (df_s['Eff2Type']=='FISH.HOUR')]\n",
    "\n",
    "df_s = df_s[(df_s['TimePeriodID']!=15) & (df_s['TimePeriodID']!= 17)]\n",
    "\n",
    "indexes = df_s[ (df_s['GearGrpCode']=='LL') & (df_s['Eff1Type']=='D.FISH') ].index\n",
    "df_s.drop(indexes, inplace=True)   # drop LL records with only D.FISH\n",
    "\n",
    "n_selected = df_s.shape[0]\n",
    "print(n_selected)\n",
    "print('ratio =', n_selected / n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SquareTypeCode GearGrpCode GearCode   Eff1Type   Eff2Type  counts\n",
      "0             1x1          BB       BB     D.FISH              27518\n",
      "1             1x1          BB       BB     D.FISH   D.AT SEA      26\n",
      "2             1x1          BB       BB     D.FISH    NO.SETS      27\n",
      "3             1x1          BB       BB  FISH.HOUR                 27\n",
      "4             1x1          BB       BB  FISH.HOUR   D.AT SEA     128\n",
      "5             1x1          BB       BB  FISH.HOUR  HOURS.SEA   50872\n",
      "6             1x1          BB       BB    NO.SETS  FISH.HOUR    3226\n",
      "7             1x1          BB      BBF     D.FISH                442\n",
      "8             1x1          LL       LL   NO.HOOKS              10967\n",
      "9             1x1          LL       LL   NO.HOOKS     D.FISH      20\n",
      "10            1x1          LL  LL-surf   NO.HOOKS                355\n",
      "11            1x1          LL     LLFB   NO.HOOKS               3601\n",
      "12            1x1          LL    LLJAP   NO.HOOKS                 27\n",
      "13            1x1          LL    LLSWO   NO.HOOKS                 38\n",
      "14            1x1          PS       PS     D.FISH              24663\n",
      "15            1x1          PS       PS     D.FISH    NO.SETS       1\n",
      "16            1x1          PS       PS  FISH.HOUR               2032\n",
      "17            1x1          PS       PS  FISH.HOUR  HOURS.SEA  317866\n",
      "18            1x1          PS       PS    NO.SETS  FISH.HOUR   11580\n",
      "19            1x1          PS      PSG     D.FISH              26472\n",
      "20            1x1          PS      PSM     D.FISH              10947\n",
      "21            5x5          BB       BB     D.FISH               5973\n",
      "22            5x5          LL       LL   NO.HOOKS              56620\n",
      "23            5x5          LL       LL   NO.HOOKS     D.FISH    1530\n",
      "24            5x5          LL       LL   NO.HOOKS    NO.SETS      24\n",
      "25            5x5          LL     LL-B   NO.HOOKS                 73\n",
      "26            5x5          LL  LL-surf   NO.HOOKS                 70\n",
      "27            5x5          LL     LLFB   NO.HOOKS               4708\n",
      "28            5x5          LL    LLJAP   NO.HOOKS                 24\n",
      "29            5x5          LL    LLSWO   NO.HOOKS               8501\n",
      "30            5x5          LL    LLSWO   NO.HOOKS   NO.TRIPS     664\n",
      "31            5x5          PS       PS     D.FISH               1786\n"
     ]
    }
   ],
   "source": [
    "sank3 = df_s.groupby(['SquareTypeCode', 'GearGrpCode', 'GearCode', 'Eff1Type', 'Eff2Type']).size().reset_index(name='counts')\n",
    "print(sank3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prioritize some types of data, excluding others, concerning:\n",
    "\n",
    "(1) square type: use only squares < 5째, i.e. only 1x1 and 5x5 squares, excluding 5x10 and 10x10;\n",
    "\n",
    "(2) fishing gear: use only bait-boat, longline and purse seine (BB, LL and PS), excluding minor gears like gillnets, trolling, trawling, sport fishing, handline and other unclassified gears;\n",
    "\n",
    "(3) effort type: use only fishing-time-related effort types, like fishing days (D.FISH) and fishing hours (FISH.HOUR) for BB and PS gears; and only number of hooks (NO.HOOKS) for LL gears.\n",
    "\n",
    "The number of datasets that meet these criteria is 103,375, from a total of 108,411, i.e. 95.35% of all datasets available are kept.\n",
    "\n",
    "(*) Each dataset has a single SquareTypeCode, a single Gear Type and a single Effort Type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridding and plotting fishing effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid 1x1 to plot\n",
    "Lon = np.arange(-46, 13, 1)\n",
    "Lat = np.arange(-31, 17, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "[1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969\n",
      " 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983\n",
      " 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n",
      " 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011\n",
      " 2012 2013 2014 2015 2016 2017 2018 2019]\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "year = df_s['YearC'].unique()\n",
    "month = np.array([np.arange(1,7,1), np.arange(7,13,1)])\n",
    "\n",
    "print(month)\n",
    "print(year)\n",
    "print(year.shape[0])\n",
    "\n",
    "###### EFFORT\n",
    "### shape = (year(64), semester(2), effort(2), lon(59), lat(47))\n",
    "### effort(2) = [eff_hour, eff_hook]\n",
    "#C_h = np.zeros((64, 2, 2, len(Lon), len(Lat)))\n",
    "\n",
    "###### CATCH\n",
    "### shape = (year(64), semester(2), catch(10), lon(59), lat(47))\n",
    "### catch(10) = [Tmh, Tmk, Tsh, Tsk, Toh, Tok, Smh, Smk, Soh, Sok]\n",
    "        ### 1st letter = T: tuna, S: shark\n",
    "        ### 2nd letter = m: major, s:small, o:other\n",
    "        ### 3rd letter = h: hour, k:hook\n",
    "C_c = np.zeros((64, 2, 10, len(Lon), len(Lat)))\n",
    "\n",
    "### method to recollect csv and reshape\n",
    "\n",
    "#np.savetxt('aaa_testeeeee.csv', np.ravel(C_h), delimiter=',')\n",
    "#print(C_h.shape)          (64, 12, 7, 59, 47)\n",
    "#C_rav = np.ravel(C_h)\n",
    "#print(C_rav.shape)        (14907648,)\n",
    "#C_rebu = np.reshape(C_rav, (64, 12, 7, 59, 47))\n",
    "#print(C_rebu.shape)       (64, 12, 7, 59, 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s['TimePeriodID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correction on problematic data, fleet ID 105GN01\n",
    "### DSetID 5258, with ~1 million D.FISH\n",
    "for i in [477347, 477348, 477349, 477350, 477351, 477353, 477354, 477355, 477356, 477357, 477358]:\n",
    "    val = df_s.at[i, 'Eff1']\n",
    "    df_s.at[i, 'Eff1'] = val / 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correction on problematic data, fleet ID 061SH00\n",
    "### DSetIDs 1862, 1933, 2199, 2273, 2345, 2404, with +100 D.FISH per month\n",
    "### in years 1989, 1990, 1994, 1995, 1996 and 1997\n",
    "prob_lines = np.hstack((np.arange(124432,124439,1), np.arange(129023,129035,1), np.arange(175637,175657), \n",
    "                        np.arange(187652,187675,1), np.arange(206977,206997,1), np.arange(228304,228316)))\n",
    "for i in prob_lines:\n",
    "    val = df_s.at[i, 'Eff1']\n",
    "    df_s.at[i, 'Eff1'] = val / 100\n",
    "    \n",
    "prob_lines = np.hstack((np.arange(151139,151172), np.arange(246949,246970), np.arange(283324,283347)))\n",
    "for i in prob_lines:\n",
    "    val = df_s.at[i, 'Eff1']\n",
    "    df_s.at[i, 'Eff1'] = val / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" df_prob1 = df_s[df_s['YearC']==1992]\\ndsets = df_prob1['DSetID'].unique()\\ndf_prob2 = df_prob1[df_prob1['DSetID'].isin(dsets[19:21])]\\npd.set_option('display.max_columns', 140)\\nprint(df_prob2.iloc[:, :21])\\n\\ndel(df_prob1, df_prob2, dsets) \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_prob1 = df_s[df_s['YearC']==1992]\n",
    "#dsets = df_prob1['DSetID'].unique()\n",
    "#df_prob2 = df_prob1[df_prob1['DSetID'].isin(dsets[19:21])]\n",
    "#pd.set_option('display.max_columns', 140)\n",
    "#print(df_prob2.iloc[:, :21])\n",
    "\n",
    "#del(df_prob1, df_prob2, dsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 1956 1\n",
      ">>>> 1957 1\n",
      ">>>> 1958 1\n",
      ">>>> 1959 1\n",
      ">>>> 1960 1\n",
      ">>>> 1961 1\n",
      ">>>> 1962 1\n",
      ">>>> 1963 1\n",
      ">>>> 1964 1\n",
      ">>>> 1965 1\n",
      ">>>> 1966 2\n",
      ">>>> 1967 4\n",
      ">>>> 1968 4\n",
      ">>>> 1969 8\n",
      ">>>> 1970 8\n",
      ">>>> 1971 8\n",
      ">>>> 1972 7\n",
      ">>>> 1973 12\n",
      ">>>> 1974 13\n",
      ">>>> 1975 14\n",
      ">>>> 1976 17\n",
      ">>>> 1977 18\n",
      ">>>> 1978 17\n",
      ">>>> 1979 20\n",
      ">>>> 1980 23\n",
      ">>>> 1981 26\n",
      ">>>> 1982 27\n",
      ">>>> 1983 29\n",
      ">>>> 1984 34\n",
      ">>>> 1985 30\n",
      ">>>> 1986 29\n",
      ">>>> 1987 27\n",
      ">>>> 1988 23\n",
      ">>>> 1989 23\n",
      ">>>> 1990 23\n",
      ">>>> 1991 24\n",
      ">>>> 1992 27\n",
      ">>>> 1993 23\n",
      ">>>> 1994 29\n",
      ">>>> 1995 30\n",
      ">>>> 1996 33\n",
      ">>>> 1997 30\n",
      ">>>> 1998 34\n",
      ">>>> 1999 40\n",
      ">>>> 2000 34\n",
      ">>>> 2001 35\n",
      ">>>> 2002 41\n",
      ">>>> 2003 37\n",
      ">>>> 2004 37\n",
      ">>>> 2005 35\n",
      ">>>> 2006 37\n",
      ">>>> 2007 37\n",
      ">>>> 2008 33\n",
      ">>>> 2009 37\n",
      ">>>> 2010 33\n",
      ">>>> 2011 33\n",
      ">>>> 2012 32\n",
      ">>>> 2013 37\n",
      ">>>> 2014 34\n",
      ">>>> 2015 32\n",
      ">>>> 2016 31\n",
      ">>>> 2017 30\n",
      ">>>> 2018 29\n",
      ">>>> 2019 26\n"
     ]
    }
   ],
   "source": [
    "C_h = np.zeros((64, 2, 2, len(Lon), len(Lat)))\n",
    "\n",
    "for yi, y in enumerate(year):                  # Treat each year, then each dataset\n",
    "    df_y = df_s[df_s['YearC']==y]\n",
    "    dsets = df_y['DSetID'].unique()\n",
    "    print('>>>>', y, len(dsets))\n",
    "    for di, d in enumerate(dsets):\n",
    "        df_d = df_y[df_y['DSetID']==d]  # until here, we are selecting a single dataset\n",
    "        #print(di+1, df_d['GearCode'].unique()[0], df_d['SquareTypeCode'].unique()[0], \n",
    "        #      df_d['Eff1Type'].unique()[0], df_d['Eff2Type'].unique()[0])\n",
    "        #print(df_d.shape)\n",
    "        \n",
    "        for mi, m in enumerate(month):\n",
    "            df_m = df_d[df_d['TimePeriodID'].isin(m)]   # selecting semester\n",
    "            df_h1 = df_m[(df_m['Eff1Type']=='FISH.HOUR')]  # Assign dataframes with one effort type each\n",
    "            df_h2 = df_m[(df_m['Eff2Type']=='FISH.HOUR')]\n",
    "            df_h3 = df_m[(df_m['Eff1Type']=='D.FISH')]\n",
    "            df_k = df_m[(df_m['Eff1Type']=='NO.HOOKS')]\n",
    "\n",
    "            #print(df_h1.shape, df_h2.shape, df_h3.shape, df_k.shape)\n",
    "            sum_h1 = df_h1.groupby(['Lon', 'Lat'], as_index=False)['Eff1'].sum()  # Sum of all effort in Lat Lon\n",
    "            sum_h2 = df_h2.groupby(['Lon', 'Lat'], as_index=False)['Eff2'].sum()\n",
    "            sum_h3 = df_h3.groupby(['Lon', 'Lat'], as_index=False)['Eff1'].sum()\n",
    "            sum_k = df_k.groupby(['Lon', 'Lat'], as_index=False)['Eff1'].sum()\n",
    "            \n",
    "            #if (df_d['SquareTypeCode'].unique()[0]=='1x1') & (df_d['Eff1Type'].unique()[0]=='D.FISH'):\n",
    "                    #hum = sum_h3[(sum_h3['Lon']==-5.5)&(sum_h3['Lat']==-15.5)]\n",
    "                    #hdo = sum_h1[(sum_h1['Lon']==-6.5)&(sum_h3['Lat']==-14.5)]\n",
    "                    #htr = sum_h2[(sum_h2['Lon']==-6.5)&(sum_h3['Lat']==-14.5)]\n",
    "                    #if not hum.empty:\n",
    "                        #print(di+1, df_d['GearCode'].unique()[0], df_d['SquareTypeCode'].unique()[0], \n",
    "                        #        df_d['Eff1Type'].unique()[0], df_d['Eff2Type'].unique()[0])\n",
    "                        #print(hum)\n",
    "                    #print('<<<                        ',sum_h3[(sum_h3['Lon']==-6.5)&(sum_h3['Lat']==-14.5)])\n",
    "                    #print('<<<                        ',sum_h1[(sum_h1['Lon']==-6.5)&(sum_h3['Lat']==-14.5)])\n",
    "                    #print('<<<                        ',sum_h2[(sum_h2['Lon']==-6.5)&(sum_h3['Lat']==-14.5)])\n",
    "\n",
    "            for ipoi, poi in sum_h1.iterrows():                   # Iterate over dataset\n",
    "                lo, la, eff = poi['Lon'], poi['Lat'], poi['Eff1']\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                    C_h[yi, mi, 0, i-2:i+3, j-2:j+3] += eff / (area * 25)  # If data is\n",
    "                else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                    C_h[yi, mi, 0, i, j] += eff / area\n",
    "                    \n",
    "                    \n",
    "            for ipoi, poi in sum_h2.iterrows():                   # Iterate over dataset\n",
    "                lo, la, eff = poi['Lon'], poi['Lat'], poi['Eff2']\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                    C_h[yi, mi, 0, i-2:i+3, j-2:j+3] += eff / (area * 25)  # If data is\n",
    "                else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                    C_h[yi, mi, 0, i, j] += eff / area\n",
    "                    \n",
    "                    \n",
    "            for ipoi, poi in sum_h3.iterrows():                   # Iterate over dataset\n",
    "                lo, la, eff = poi['Lon'], poi['Lat'], poi['Eff1']                \n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                    C_h[yi, mi, 0, i-2:i+3, j-2:j+3] += eff*24 / (area * 25)  # If data is\n",
    "                else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                    C_h[yi, mi, 0, i, j] += eff*24 / area        \n",
    "                    \n",
    "            \n",
    "            for ipoi, poi in sum_k.iterrows():                   # Iterate over dataset\n",
    "                lo, la, eff = poi['Lon'], poi['Lat'], poi['Eff1']\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                    C_h[yi, mi, 1, i-2:i+3, j-2:j+3] += eff / (area * 25)  # If data is\n",
    "                else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                    C_h[yi, mi, 1, i, j] += eff / area\n",
    "\n",
    "\n",
    "C_h[C_h == 0] = np.nan    # Replace all zeros by NaNs, so they stay white in the map\n",
    "\n",
    "np.savetxt('10mar22_eff.csv', np.ravel(C_h), delimiter=',')\n",
    "del(C_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tm = species_list[:10]\n",
    "Ts = species_list[10:24]\n",
    "To = species_list[24:31]\n",
    "Sm = species_list[31:34]\n",
    "So = species_list[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_s['CatchUnit'].value_counts())\n",
    "print(df_s['DSetTypeID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sank4 = df_s.groupby(['DSetTypeID', 'YearC']).size().reset_index(name='counts')\n",
    "pd.set_option('display.max_rows', 173)\n",
    "print(sank4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_total, n_selected)\n",
    "df_kg2 = df_s[df_s['DSetTypeID'].isin(['.w', 'nw'])]\n",
    "df_kg = df_s[df_s['CatchUnit']=='kg']\n",
    "del(df_s)\n",
    "n_kg2 = df_kg2.shape[0]\n",
    "n_kg = df_kg.shape[0]\n",
    "del(df_kg2)\n",
    "print('ratio kg kg2 =', n_kg / n_kg2)\n",
    "print(n_kg)\n",
    "print('ratio from total =', n_kg2 / n_total)\n",
    "print('ratio from selected =', n_kg2 / n_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sank6 = df_kg2.groupby(['DSetTypeID', 'YearC']).size().reset_index(name='counts')\n",
    "print(sank6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sank5 = df_kg.groupby(['DSetTypeID', 'YearC']).size().reset_index(name='counts')\n",
    "print(sank5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for yi, y in enumerate(year):                  # Treat each year, then each dataset\n",
    "    df_y = df_kg[df_kg['YearC']==y]\n",
    "    dsets = df_y['DSetID'].unique()\n",
    "    print('>>>>', y, len(dsets))\n",
    "    for di, d in enumerate(dsets):\n",
    "        df_d = df_y[df_y['DSetID']==d]  # until here, we are selecting a single dataset\n",
    "        print(di+1, df_d['GearCode'].unique()[0], df_d['SquareTypeCode'].unique()[0], \n",
    "              df_d['Eff1Type'].unique()[0], df_d['Eff2Type'].unique()[0])\n",
    "        #print(df_d.shape)\n",
    "        for mi, m in enumerate(month):\n",
    "            df_m = df_d[df_d['TimePeriodID'].isin(m)]   # selecting semester\n",
    "            sum_Tm = df_m.groupby(['Lon', 'Lat'], as_index=False)[Tm].sum()  # Sum of all Capture in Lat Lon\n",
    "            sum_Ts = df_m.groupby(['Lon', 'Lat'], as_index=False)[Ts].sum()\n",
    "            sum_To = df_m.groupby(['Lon', 'Lat'], as_index=False)[To].sum()\n",
    "            sum_Sm = df_m.groupby(['Lon', 'Lat'], as_index=False)[Sm].sum()\n",
    "            sum_So = df_m.groupby(['Lon', 'Lat'], as_index=False)[So].sum()\n",
    "                                          \n",
    "            for ipoi, poi in sum_Tm.iterrows():                   # Iterate over dataset                \n",
    "                lo, la, cap = poi['Lon'], poi['Lat'], poi.iloc[2:].sum()\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['GearGrpCode'].unique()[0] == 'LL':\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 1, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 1, i, j] += cap / area\n",
    "                else:\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 0, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 0, i, j] += cap / area\n",
    "                        \n",
    "                        \n",
    "            for ipoi, poi in sum_Ts.iterrows():                   # Iterate over dataset                \n",
    "                lo, la, cap = poi['Lon'], poi['Lat'], poi.iloc[2:].sum()\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['GearGrpCode'].unique()[0] == 'LL':\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 3, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 3, i, j] += cap / area\n",
    "                else:\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 2, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 2, i, j] += cap / area     \n",
    "                        \n",
    "                        \n",
    "            for ipoi, poi in sum_To.iterrows():                   # Iterate over dataset                \n",
    "                lo, la, cap = poi['Lon'], poi['Lat'], poi.iloc[2:].sum()\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['GearGrpCode'].unique()[0] == 'LL':\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 5, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 5, i, j] += cap / area\n",
    "                else:\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 4, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 4, i, j] += cap / area           \n",
    "                        \n",
    "                        \n",
    "            for ipoi, poi in sum_Sm.iterrows():                   # Iterate over dataset                \n",
    "                lo, la, cap = poi['Lon'], poi['Lat'], poi.iloc[2:].sum()\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['GearGrpCode'].unique()[0] == 'LL':\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 7, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 7, i, j] += cap / area\n",
    "                else:\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 6, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 6, i, j] += cap / area        \n",
    "                        \n",
    "                        \n",
    "            for ipoi, poi in sum_So.iterrows():                   # Iterate over dataset                \n",
    "                lo, la, cap = poi['Lon'], poi['Lat'], poi.iloc[2:].sum()\n",
    "                i, j = np.where(Lon==lo+.5)[0][0], np.where(Lat==la+.5)[0][0]\n",
    "                area = 111.195 * 111.195 * np.cos(la*np.pi/180)\n",
    "                \n",
    "                if df_m['GearGrpCode'].unique()[0] == 'LL':\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 9, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 9, i, j] += cap / area\n",
    "                else:\n",
    "                    if df_m['SquareTypeCode'].unique()[0] == '5x5':\n",
    "                        C_c[yi, mi, 8, i-2:i+3, j-2:j+3] += cap / (area * 25)  # If data is\n",
    "                    else:                                # 5x5, distribute effort over 25 1x1 cells\n",
    "                        C_c[yi, mi, 8, i, j] += cap / area\n",
    "                        \n",
    "del(df_kg)    \n",
    "C_c[C_c == 0] = np.nan\n",
    "\n",
    "np.savetxt('10mar22_cap.csv', np.ravel(C_c), delimiter=',')\n",
    "del(C_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
